{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Regressor, Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_feature(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree', # bgtree基于树的模型，gbliner线性模型\n",
    "              'objective':'rank:pairwise', #最小化的损失函数\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02, #类似学习率，通过减少每一步的权重，可以提高模型的鲁棒性。典型值为0.01-0.2\n",
    "              'max_depth': 5,  # 树的最大深度，默认为6，避免过拟合，越大越容易过拟合\n",
    "              'colsample_bytree': 0.7, # 生成树时进行的列采样，取值0.5-1\n",
    "              'subsample': 0.7,#随机采样训练样本，减小会避免过拟合，过小会欠拟合，取值0.5-1\n",
    "              'min_child_weight': 1,  # 最小叶子节点样本权重和，默认为1，避免过拟合，如果这个值过高，会导致欠拟合\n",
    "              'seed': 1111, # 随机数的种子\n",
    "              'silent':1 ## 设置成1则没有运行信息输出，默认为0.\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=800)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def xgb_feature2(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.015,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'seed': 11,\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=1200)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def xgb_feature3(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.01,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'seed': 1,\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=2000)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def et_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = ExtraTreesClassifier(max_features = 'log2', n_estimators = 1000 , n_jobs = -1).fit(X_train,y_train)\n",
    "    return model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def gbdt_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = GradientBoostingClassifier(learning_rate = 0.02, max_features = 0.7, n_estimators = 700 , max_depth = 5)\n",
    "    model.fit(X_train,y_train)\n",
    "    predict = model.predict_proba(X_test)[:,1]\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def logistic_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = LogisticRegression(penalty = 'l2').fit(X_train,y_train)\n",
    "    return model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_feature(X_train, y_train, X_test, y_test=None):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train,categorical_feature={'sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry'})\n",
    "    lgb_test = lgb.Dataset(X_test,categorical_feature={'sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry'})\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'num_leaves': 25,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf':5,\n",
    "        'max_bin':200,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "    gbm = lgb.train(params,lgb_train,num_boost_round=2000)\n",
    "    predict = gbm.predict(X_test)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('AI_risk_train_V3.0/train_data.csv',encoding='gb2312')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用测试集计算AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56007, 105)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_train_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('AI_risk_train_V3.0/train_data.csv',encoding='gb2312')\n",
    "# dummy_fea1 = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry']\n",
    "# dummy_df1 = pd.get_dummies(train_data.loc[:,dummy_fea])\n",
    "# train_data_copy1 = pd.concat([train_data,dummy_df],axis=1)\n",
    "# train_data_copy1 = train_data_copy1.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "# train_data_copy1 = train_data_copy1.fillna(0)\n",
    "# valid_train_data1 = train_data_copy1.drop(dummy_fea,axis=1)\n",
    "# valid_train_train1 = valid_train_data1[(valid_train_data1.loan_year <= 2017) & (valid_train_data1.loan_month < 4)]\n",
    "# valid_train_test1 = valid_train_data1[(valid_train_data1.loan_year >= 2017) & (valid_train_data1.loan_month >= 4)]\n",
    "# valid_train_x1 = valid_train_train1.drop(['target'],axis=1)\n",
    "# valid_test_x1 = valid_train_test1.drop(['target'],axis=1)\n",
    "\n",
    "# xgb_dataset1 = Dataset(X_train=valid_train_x1,y_train=valid_train_train1['target'],X_test=valid_test_x1,y_test=None,use_cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR valid auc 0.6895657003902818\n"
     ]
    }
   ],
   "source": [
    "lr_redict_result = logistic_model(valid_train_x1,valid_train_train1['target'].values,valid_test_x1,None)\n",
    "print('LR valid auc',roc_auc_score(valid_train_test['target'].values,lr_redict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('AI_risk_train_V3.0/train_data.csv',encoding='gb2312')\n",
    "dummy_fea = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry']\n",
    "for _fea in dummy_fea:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data[_fea].tolist())\n",
    "    train_data[_fea] = le.transform(train_data[_fea].tolist())\n",
    "train_data_copy1 = train_data.copy()\n",
    "valid_train_data1 = train_data_copy1.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "valid_train_data1 = valid_train_data1.fillna(0)\n",
    "valid_train_train1 = valid_train_data1[(valid_train_data1.loan_year <= 2017) & (valid_train_data1.loan_month < 4)]\n",
    "valid_train_test1 = valid_train_data1[(valid_train_data1.loan_year >= 2017) & (valid_train_data1.loan_month >= 4)]\n",
    "valid_train_x1 = valid_train_train1.drop(['target'],axis=1)\n",
    "valid_test_x1 = valid_train_test1.drop(['target'],axis=1)\n",
    "\n",
    "dataset1 = Dataset(valid_train_x1,valid_train_train1['target'],valid_test_x1,use_cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb valid auc： 0.8195550244976295\n"
     ]
    }
   ],
   "source": [
    "lgb_redict_result = lgb_feature(valid_train_x1,valid_train_train1['target'].values,valid_test_x1,None)\n",
    "print('lgb valid auc：',roc_auc_score(valid_train_test1['target'].values,lgb_redict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = Regressor(dataset=dataset1, estimator=xgb_feature,name='xgb',use_cache=False)\n",
    "model_xgb2 = Regressor(dataset=dataset1, estimator=xgb_feature2,name='xgb2',use_cache=False)\n",
    "model_xgb3 = Regressor(dataset=dataset1, estimator=xgb_feature3,name='xgb3',use_cache=False)\n",
    "model_lgb = Regressor(dataset=dataset1, estimator=lgb_feature,name='lgb',use_cache=False)\n",
    "model_gbdt = Regressor(dataset=dataset1, estimator=gbdt_model,name='gbdt',use_cache=False)\n",
    "# Stack5个模型\n",
    "pipeline = ModelsPipeline(model_xgb,model_xgb2,model_xgb3,model_lgb,model_gbdt)\n",
    "stack_ds = pipeline.stack(k=5, seed=111, add_diff=False, full_test=True)\n",
    "#第二层使用lr模型stack\n",
    "stacker = Regressor(dataset=stack_ds, estimator=LinearRegression,parameters={'fit_intercept': False})\n",
    "stacking_predict_result = stacker.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking valid auc： 0.8134186440056409\n"
     ]
    }
   ],
   "source": [
    "print('stacking valid auc：',roc_auc_score(valid_train_test1['target'].values,stacking_predict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x0000022813EC8908>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\software\\python\\programs\\python\\python35\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blending valid auc： 0.8139588698988814\n"
     ]
    }
   ],
   "source": [
    "blend_ds = pipeline.blend(proportion=0.2, seed=111)\n",
    "blender = Regressor(dataset=blend_ds, estimator=LinearRegression)\n",
    "blending_predict_result = blender.predict()\n",
    "\n",
    "print('blending valid auc：',roc_auc_score(valid_train_test1['target'].values,blending_predict_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb数据集处理\n",
    "# 对文本类型LabelEncoder()转换\n",
    "\n",
    "train_data = pd.read_csv('AI_risk_train_V3.0/train_data.csv',encoding='gb2312')\n",
    "train_data = train_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "train_data = train_data.fillna(0)\n",
    "\n",
    "test_data = pd.read_csv('AI_risk_test_V3.0/test_data.csv',encoding='gb2312')\n",
    "test_data = test_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_test_data = pd.concat([train_data,test_data],axis=0,ignore_index = True)\n",
    "train_test_data = train_test_data.fillna(0)\n",
    "train_data = train_test_data.iloc[:train_data.shape[0],:]\n",
    "test_data = train_test_data.iloc[train_data.shape[0]:,:]\n",
    "dummy_fea = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry']\n",
    "for _fea in dummy_fea:\n",
    "    #print(_fea)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data[_fea].tolist() + test_data[_fea].tolist())\n",
    "    tmp = le.transform(train_data[_fea].tolist() + test_data[_fea].tolist())\n",
    "    train_data[_fea] = tmp[:train_data.shape[0]]\n",
    "    test_data[_fea] = tmp[train_data.shape[0]:]\n",
    "train_x = train_data.drop(['target'],axis=1)\n",
    "test_x = test_data.drop(['target'],axis=1)\n",
    "lgb_dataset = Dataset(train_x,train_data['target'],test_x,use_cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb数据集处理\n",
    "# 对文本类型one-hot编码\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('AI_risk_train_V3.0/train_data.csv',encoding='gb2312')\n",
    "train_data = train_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = pd.read_csv('AI_risk_test_V3.0/test_data.csv',encoding='gb2312')\n",
    "test_data = test_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "test_data = test_data.fillna(0)\n",
    "dummy_fea = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry']\n",
    "train_test_data = pd.concat([train_data,test_data],axis=0,ignore_index = True)\n",
    "train_test_data = train_test_data.fillna(0)\n",
    "dummy_df = pd.get_dummies(train_test_data.loc[:,dummy_fea])\n",
    "train_test_data = pd.concat([train_test_data,dummy_df],axis=1)\n",
    "train_test_data = train_test_data.drop(dummy_fea,axis=1)\n",
    "train_train = train_test_data.iloc[:train_data.shape[0],:]\n",
    "test_test = train_test_data.iloc[train_data.shape[0]:,:]\n",
    "train_train_x = train_train.drop(['target'],axis=1)\n",
    "test_test_x = test_test.drop(['target'],axis=1)\n",
    "xgb_dataset = Dataset(X_train=train_train_x,y_train=train_train['target'],X_test=test_test_x,y_test=None,use_cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heamy \n",
    "# 1.5h\n",
    "model_xgb = Regressor(dataset=xgb_dataset, estimator=xgb_feature,name='xgb',use_cache=False)\n",
    "model_xgb2 = Regressor(dataset=xgb_dataset, estimator=xgb_feature2,name='xgb2',use_cache=False)\n",
    "model_xgb3 = Regressor(dataset=xgb_dataset, estimator=xgb_feature3,name='xgb3',use_cache=False)\n",
    "model_lgb = Regressor(dataset=lgb_dataset, estimator=lgb_feature,name='lgb',use_cache=False)\n",
    "model_gbdt = Regressor(dataset=xgb_dataset, estimator=gbdt_model,name='gbdt',use_cache=False)\n",
    "\n",
    "pipeline = ModelsPipeline(model_xgb,model_xgb2,model_xgb3,model_lgb,model_gbdt)\n",
    "stack_ds = pipeline.stack(k=5, seed=111, add_diff=False, full_test=True)\n",
    "stacker = Regressor(dataset=stack_ds, estimator=LinearRegression,parameters={'fit_intercept': False})\n",
    "predict_result = stacker.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv('AI_risk_test_V3.0/test_list.csv',parse_dates = ['appl_sbm_tm'])\n",
    "ans['PROB'] = predict_result\n",
    "ans = ans.drop(['appl_sbm_tm'],axis=1)\n",
    "minmin, maxmax = min(ans['PROB']),max(ans['PROB'])\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:'%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv('result/ans_stacking.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
